{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "plotwidth=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from natsort import natsort_keygen\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import psycopg2\n",
    "import netrc\n",
    "import re\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals\n",
    "\n",
    "A few general variable about where to find stuff. Adapt to your own needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '.'\n",
    "plotdir = 'plots'\n",
    "vpipe_working = 'working' # V-pipe's working directory\n",
    "\n",
    "# Input\n",
    "tally_mut = os.path.join(datadir, 'tallymut_line.tsv')\n",
    "cooc_table = os.path.join(vpipe_working, 'ww-cooc.csv')\n",
    "viollier_data = os.path.join(datadir, 'viollier_data.csv')\n",
    "data_per_day_and_canton = os.path.join(datadir, 'data_per_day_and_canton2.csv')\n",
    "\n",
    "# Select\n",
    "start_date = '2020-12-08'\n",
    "todaydate = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "cities_list=['Altenrhein (SG)', 'Chur (GR)', 'Genève (GE)', 'Laupen (BE)',\n",
    "       'Lausanne (VD)', 'Lugano (TI)', 'Zürich (ZH)']\n",
    "variants_list=['UK','ZA','BR','C36','IN1','IN2','IN3']\n",
    "variants_pangolin={'UK':'B.1.1.7','ZA':'B.1.351','BR':'P.1','C36':'C.36.3','IN1':'B.1.617.1','IN2':'B.1.617.2','IN3':'B.1.617.3'}\n",
    "exclusive_list=['ZA','BR'] # list of variants where we should apply filtering\n",
    "exclude_from=['UK','ZA','BR' ] #,'IN2'] #,'C36','IN1','IN2','IN3'] # filter against these variants\n",
    "# Output\n",
    "update_data_file = os.path.join(datadir, 'ww_update_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for resampling\n",
    "num_resample=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegEx used to decode plantcode and date from sample name\n",
    "# should return a dict (named groups):\n",
    "#  - plant: the code of the wastewater plant (if plant_name_tsv is provided, it will be looked up for a full name)\n",
    "#  - year, month, day: used to make a time code for the time-serie\n",
    "rxname=re.compile('(?P<plant>\\d+)_(?P<year>20\\d{2})_(?:(?:(?P<month>[01]?\\d)_(?P<day>[0-3]?\\d))|(?:R_(?P<repeat>\\d+)))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WWTP sequencing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(tally_mut, sep='\\t', parse_dates=['date'])\n",
    "\n",
    "df['mutation'] = df['pos'].astype(str) + df['base']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cooc_raw = pd.read_csv(cooc_table, sep=',') #, index_col=['sample','batch'])\n",
    "\n",
    "#df['mutation'] = df['pos'].astype(str) + df['base']\n",
    "\n",
    "df_cooc_raw#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for duplicated samples with suffixes\n",
    "df[(~df['plantname'].isna()) & (df['date'] >= start_date) & (df['base'] != '-') & df.duplicated(subset=['plantname','date','batch','mutation'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK workaround for a duplicated sample\n",
    "df.loc[df['sample']=='A1_05_2021_05_19_CATTCGGA-TTTCCATC','batch']='20210604_JN8TR_CATTCGGA-TTTCCATC'\n",
    "df_cooc_raw.loc[df_cooc_raw['sample']=='A1_05_2021_05_19_CATTCGGA-TTTCCATC','batch']='20210604_JN8TR_CATTCGGA-TTTCCATC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plantcode and date from mut_table to cooc\n",
    "df_map=df[['sample','batch','plantname','date']].drop_duplicates(ignore_index=True).set_index(['sample','batch'])\n",
    "df_cooc=df_cooc_raw.merge(df_map[['plantname','date']], how='left', on=['sample','batch'], copy=False, validate='many_to_one').set_index(['plantname','date','batch'])\n",
    "df_cooc#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO load actual amplicon information\n",
    "\n",
    "amplicons = {\n",
    "    'IN1': { 76: ['22917G', '23012C'], },\n",
    "    'IN3': { 76: ['22917G', '23012C'], },\n",
    "    'IN2': { 76: ['22917G', '22995A'],\n",
    "             91: ['27638C', '27752T'], },\n",
    "    'BR':  { 71: ['21621A', '21638T'],  # ,'21614T'], # common mutations are removed from the plot\n",
    "             # 95: ['28877T',  '28878C'], # not part of the signature mutations\n",
    "           },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutfilter(var, exclusive=False):\n",
    "    # if exclusive is set on true, it will filter only those mutation which are variant specific and DO NOT show up in other variants\n",
    "    # e.g.: used to exclude 23063T (V501Y) as all variant have it\n",
    "    return (df[exclude_from].fillna(0) == ['mut' if v==var else 0 for v in exclude_from]).all(axis=1) if exclusive else (df[var] == \"mut\")\n",
    "mutfilter('UK', exclusive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = {}\n",
    "amp_col = {}\n",
    "for var in tqdm(variants_list):\n",
    "    df_wide[var] = (\n",
    "        # for the remaining mutations\n",
    "        df[(~df['plantname'].isna()) & (df['date'] >= start_date) & (df['base'] != '-') & (mutfilter(var, exclusive=(var in exclusive_list)))]\n",
    "         .pivot(index=['plantname', 'date', 'batch'], columns=['mutation'], values='frac')\n",
    "         .sort_index(axis=1, key=natsort_keygen())\n",
    "    )\n",
    "    # add amplicons\n",
    "    if var in amplicons:\n",
    "        amp_col[var] = []\n",
    "        for amp,muts in amplicons[var].items():\n",
    "            aname =  f\"Amp {amp}\"\n",
    "            df_wide[var].insert(loc=1+df_wide[var].columns.get_loc(muts[-1]), column=aname, value=df_cooc.loc[(df_cooc['amplicon'] == amp) & (df_cooc[var]==1)].loc[df_wide[var].index,'frac'], allow_duplicates=False)\n",
    "            amp_col[var] += [ aname ]\n",
    "\n",
    "df_wide['IN2'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide['IN2'].loc['Zürich (ZH)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_col['IN2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_cov = (\n",
    "    df[(~df['plantname'].isna()) & (df['date'] >= '2020-12-08') & (df['base'] != '-') & (df['UK'] == \"mut\")]\n",
    "    .pivot(index=['plantname', 'date', 'batch'], columns=['mutation'], values='cov')\n",
    "    .sort_index(axis=1, key=natsort_keygen())\n",
    ")\n",
    "\n",
    "df_wide_counts = (\n",
    "    df[(~df['plantname'].isna()) & (df['date'] >= '2020-12-08') & (df['base'] != '-') & (df['UK'] == \"mut\")]\n",
    "    .pivot(index=['plantname', 'date', 'batch'], columns=['mutation'], values='var')\n",
    "    .sort_index(axis=1, key=natsort_keygen())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in tqdm(cities_list, desc='Cities', position=0):\n",
    "    for var in tqdm(variants_list, desc='Variants', position=1):\n",
    "        tmp=df_wide[var].loc[city]\n",
    "\n",
    "        # drawing box decorations:\n",
    "        #      22917G┐\n",
    "        #      22995A┤\n",
    "        # Amplicon 76┘\n",
    "        if var in amplicons:\n",
    "            rmap={}\n",
    "            # mutations\n",
    "            for amp,muts in amplicons[var].items():\n",
    "                box = ' ┐'\n",
    "                for m in muts:\n",
    "                    rmap[m]=f\"{m}{box}\"\n",
    "                    box = '┤'\n",
    "            # amplicons\n",
    "            for a in amp_col[var]:\n",
    "                    rmap[a]=f\"{a}┘\"\n",
    "            tmp.rename(columns=rmap,inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(plotwidth,5)) # 17\n",
    "        sns.heatmap(\n",
    "            data=tmp.T.applymap(lambda x: np.nan if pd.isna(x) else x),\n",
    "            annot=tmp.T,\n",
    "            fmt='.1g',\n",
    "            square=False, cbar=False,\n",
    "            cmap=sns.color_palette(\"viridis\", as_cmap=True)\n",
    "        ).set_title(f\"{city} - {variants_pangolin[var]}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do resampling confint for lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_fn(x, nsamples):\n",
    "    return np.array([np.mean(np.random.choice(x[np.isnan(x)==False],\n",
    "                                              x[np.isnan(x)==False].shape[0],\n",
    "                                              replace=True)) \\\n",
    "            for i in range(nsamples)])\n",
    "\n",
    "lowess = sm.nonparametric.lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    #warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    #warnings.filterwarnings(action='ignore', category=RuntimeWarning, message='Mean of empty slice')\n",
    "    #warnings.simplefilter(action='once')\n",
    "\n",
    "    r_df={}\n",
    "    yres={}\n",
    "    yres2={}\n",
    "    agg1={}\n",
    "    agg2={}\n",
    "    agg3={}\n",
    "    for city in tqdm(cities_list, desc='Cities', position=0):\n",
    "        r_df[city]={}\n",
    "        yres[city]={}\n",
    "        yres2[city]={}\n",
    "        agg1[city]={}\n",
    "        agg2[city]={}\n",
    "        agg3[city]={}\n",
    "        for var in tqdm(variants_list, desc='Variants', position=1, leave=False):\n",
    "            r_df[city][var] = df_wide[var].loc[city].dropna(axis=1, how='all').T  # .loc[28111:28111]\n",
    "            # TODO be more clever with amplicons (for now we're just ignoring them)\n",
    "            tmp = r_df[city][var].drop(index=amp_col[var],inplace=False,errors='ignore') if var in amp_col else r_df[city][var]\n",
    "            yres[city][var] = tmp.T.apply(lambda x: resample_fn(x, num_resample), 1)\n",
    "\n",
    "            yres2[city][var] = pd.DataFrame(np.array([i for i in yres[city][var].values]))\n",
    "            yres2[city][var].index = yres[city][var].index\n",
    "\n",
    "            agg1[city][var] = yres2[city][var].groupby('date').agg('mean').asfreq('D')\n",
    "            agg2[city][var] = agg1[city][var].apply(lambda x: x.rolling(7, min_periods=1).mean(), 0)\n",
    "            print(city, var, yres2[city][var].shape[0], 20./yres2[city][var].shape[0], np.clip(20./yres2[city][var].shape[0], 0, 2./3))\n",
    "            agg3[city][var] = agg1[city][var].apply(lambda x: lowess(x, np.arange(x.shape[0]).astype('float64'),\n",
    "                                                    xvals = np.arange(x.shape[0]).astype('float64'),\n",
    "                                                    frac= np.clip(20./yres2[city][var].shape[0], 0, 2./3), it=0), 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities_list:\n",
    "    for var in variants_list:\n",
    "        print(f\"{city}-{var}:\\t{yres2[city][var].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df={}\n",
    "# import matplotlib.gridspec as gridspec\n",
    "for city in tqdm(cities_list, desc='Cities', position=0):\n",
    "    m_df[city]={}\n",
    "    for var in tqdm(variants_list, desc='Variants', position=1, leave=False):\n",
    "        m_df[city][var] = r_df[city][var].T.groupby(\"date\").agg(\"mean\").asfreq('D').T\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, figsize=(plotwidth, 10), sharex=False)\n",
    "        ax = [ax]\n",
    "\n",
    "        # TODO be more clever with amplicons (for now we're just ignoring them)\n",
    "        tmp = m_df[city][var].drop(index=amp_col[var],inplace=False,errors='ignore') if var in amp_col else m_df[city][var]\n",
    "        xvals = tmp.T.index#.astype(\"str\")\n",
    "        yvals = tmp.apply(np.mean, 0)\n",
    "\n",
    "        ###\n",
    "\n",
    "        sns.lineplot(x=xvals, y=np.clip(agg3[city][var].apply(np.mean, 1), 0., 1.), ax=ax[0], label=\"wastewater lowess smoothing\")\n",
    "        ax[0].fill_between(xvals,\n",
    "                           np.clip(agg3[city][var].apply(lambda x: np.percentile(x, 5), 1).interpolate(), 0, 1),\n",
    "                           np.clip(agg3[city][var].apply(lambda x: np.percentile(x, 95), 1).interpolate(), 0, 1),\n",
    "                           alpha=.3)\n",
    "        ##<-Here\n",
    "        #sns.lineplot(x=xvals[:66], y=np.nanmean(np.array(ZHclinlowess), axis=0),\n",
    "        #             ax=ax[0], label=\"cantonal clinical lowess smoothing\")\n",
    "        #ax[0].fill_between(xvals[:66],\n",
    "        #                     np.nanpercentile(np.array(ZHclinlowess), q=2.5, axis=0),\n",
    "        #                     np.nanpercentile(np.array(ZHclinlowess), q=97.5, axis=0),\n",
    "        #                     alpha=.3)\n",
    "        #\n",
    "        #sns.barplot(x=xvals[:66], y=viollierZH[viollierZH[\"date\"].isin(xvals)][\"frac\"],\n",
    "        #            ax=ax[0], label=\"cantonal clinical empirical frequencies\", color=\"orange\", alpha=0.5)\n",
    "\n",
    "\n",
    "        # ax[0].set_ylim((0,0.125))\n",
    "        ax[0].set_xlim((np.datetime64(start_date), np.datetime64(todaydate)))\n",
    "        ax[0].set_ylabel(f\"frac. {variants_pangolin[var]}\")\n",
    "        ax[0].legend(loc=\"upper left\")\n",
    "        ax[0].set_title(f\"{city}: relative {variants_pangolin[var]} prevalence estimates from wastewater samples\\n compared to relative prevalence estimates from cantonal clinical samples\")\n",
    "        # ax[0].set_xticks([\"2020-12-15\", \"2021-01-01\", \"2021-01-15\", \"2021-02-01\", \"2021-02-15\"])\n",
    "        #ax[0].set_xticklabels(labels=xvals, rotation=90, ha='center')\n",
    "\n",
    "        # plt.savefig(\"plots/ZurPlot2.pdf\", bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data for covSPECTRUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_data={ }\n",
    "tdf={city:{}  for city in cities_list}\n",
    "tdf_mat={city:{}  for city in cities_list}\n",
    "\n",
    "for var in tqdm(variants_list, desc='Variants', position=0):\n",
    "    update_data[variants_pangolin[var]] = { }\n",
    "    for city in tqdm(cities_list, desc='Cities', position=1, leave=False):\n",
    "        tdf[city][var] = agg3[city][var].apply(lambda x: {\"proportion\":np.clip(np.mean(x), 0., 1.),\n",
    "                                                          \"proportionLower\":np.clip(np.percentile(x, 5), 0., 1.),\n",
    "                                                          \"proportionUpper\":np.clip(np.percentile(x, 95), 0., 1.)},\n",
    "                                               axis=1, result_type ='expand')\n",
    "        tdf[city][var] = tdf[city][var].reset_index()\n",
    "        tdf[city][var][\"date\"] = tdf[city][var][\"date\"].astype(\"str\")\n",
    "\n",
    "        tdf_mat[city][var] = m_df[city][var].T.melt(ignore_index=False, var_name=\"nucMutation\", value_name=\"proportion\").reset_index()\n",
    "        tdf_mat[city][var][\"date\"] = tdf_mat[city][var][\"date\"].astype(\"str\")\n",
    "        # drawing box decorations:\n",
    "        # ┌22917G\n",
    "        # ├22995A\n",
    "        # └Amplicon 76\n",
    "        if var in amplicons:\n",
    "            # mutations\n",
    "            for amp,muts in amplicons[var].items():\n",
    "                box = '┌'\n",
    "                for m in muts:\n",
    "                    tdf_mat[city][var].loc[tdf_mat[city][var][\"nucMutation\"]==m,\"nucMutation\"]=f\"{box}{m}\"\n",
    "                    box = '├'\n",
    "            for a in amp_col[var]:\n",
    "                    tdf_mat[city][var].loc[tdf_mat[city][var][\"nucMutation\"]==a,\"nucMutation\"]=f\"└{a}\"\n",
    "\n",
    "        update_data[variants_pangolin[var]][city] = {\n",
    "            #\"updateDate\": todaydate,\n",
    "            \"timeseriesSummary\": [dict(tdf[city][var].iloc[i,]) for i in range(tdf[city][var].shape[0])],\n",
    "            \"mutationOccurrences\": [dict(tdf_mat[city][var].iloc[i,]) for i in range(tdf_mat[city][var].shape[0])]\n",
    "        }\n",
    "\n",
    "import json\n",
    "with open('ww_update_data.json', 'w') as file:\n",
    "     file.write(json.dumps(update_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New regression\n",
    "\n",
    "If you want to use the new regressing this it the point where you would need to switch to the [new `ww_smoothing_regression.ipynb` notebook](ww_smoothing_regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Cov-Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbhost='id-hdb-psgr-cp61.ethz.ch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from netrc\n",
    "dbuser,dbpass=netrc.netrc().authenticators(dbhost)[0::2]\n",
    "\n",
    "# alternative: input box\n",
    "#dbuser = input(f\"Enter username for database {dbhost}:\\n\")\n",
    "#dbpass = input(f\"Enter password for user {dbuser}:\\n\")\n",
    "\n",
    "# alternative: enviro\n",
    "#dbuser = os.environ['DB_USERNAME'],\n",
    "#dbpass = os.environ['DB_PASSWORD'],\n",
    "\n",
    "dbuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconn = psycopg2.connect(\n",
    "    host=dbhost,\n",
    "    database='sars_cov_2',\n",
    "    user=dbuser,\n",
    "    password=dbpass,\n",
    "    port='5432'\n",
    ")\n",
    "dbconn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = dbconn.cursor()\n",
    "cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variants_list:\n",
    "    for city in cities_list:\n",
    "        pango=variants_pangolin[var]\n",
    "        cur.execute(\"\"\"\n",
    "        DO $$\n",
    "        BEGIN\n",
    "         IF EXISTS (SELECT ww.data FROM public.spectrum_waste_water_result AS ww WHERE ww.variant_name=%(var)s AND ww.location=%(city)s) THEN\n",
    "          UPDATE public.spectrum_waste_water_result AS ww SET data=%(data)s WHERE ww.variant_name=%(var)s AND ww.location=%(city)s;\n",
    "         ELSE\n",
    "          INSERT INTO public.spectrum_waste_water_result (variant_name, location, data)\n",
    "          VALUES(%(var)s, %(city)s, %(data)s);\n",
    "         END IF;\n",
    "        END\n",
    "        $$\n",
    "        \"\"\", {'data': json.dumps(update_data[pango][city]).replace('NaN','null'), 'var': pango, 'city': city})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abort DB update !\n",
    "dbconn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to DB !\n",
    "dbconn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "dbconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick checks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plantname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plantcode.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.plantname == \"Bioggio (TI)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.loc[\"Bioggio (TI)\"].index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.loc[\"Ski-resort\"].index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load national viollier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viollier = pd.read_csv(viollier_data)\n",
    "viollier.index = pd.to_datetime(viollier[\"year\"].astype(str), format='%Y') +\\\n",
    "    pd.to_timedelta(((viollier[\"week\"]-1).mul(7)+3-7).astype(str) + ' days')\n",
    "viollier[\"frac_b117\"] = viollier[\"b117\"] / viollier[\"n\"]\n",
    "viollier[\"p_pseudo\"] = (viollier[\"b117\"] + 1) / viollier[\"n\"]\n",
    "viollier[\"error\"] = 1.96*np.sqrt(viollier[\"p_pseudo\"]*(1-viollier[\"p_pseudo\"])/viollier[\"n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cantonal viollier data and aggregate by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viollier2 = pd.read_csv(data_per_day_and_canton)\n",
    "viollier2[\"date\"] = pd.DatetimeIndex(viollier2[\"date\"])\n",
    "viollier2[\"week\"] = viollier2.date.dt.strftime('%W')\n",
    "viollier2[\"year\"] = viollier2.date.dt.strftime('%Y')\n",
    "viollier2[\"date_week\"] = pd.to_datetime(viollier2[\"year\"].astype(str), format='%Y') +\\\n",
    "    pd.to_timedelta(((viollier2[\"week\"].astype(\"int\")-1).mul(7)+3-7).astype(str) + ' days')\n",
    "\n",
    "viollier2_sum = viollier2.groupby([\"date_week\", \"canton\"], as_index=False).agg(\"sum\")\n",
    "viollier2_sum[\"frac_b117\"] = viollier2_sum[\"b117\"] / viollier2_sum[\"sequenced\"]\n",
    "viollier2_sum[\"p_pseudo\"] = (viollier2_sum[\"b117\"] + 1) / (viollier2_sum[\"sequenced\"] + 2)\n",
    "viollier2_sum[\"error\"] = 1.96*np.sqrt(viollier2_sum[\"p_pseudo\"]*(1-viollier2_sum[\"p_pseudo\"])/(viollier2_sum[\"sequenced\"]+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viollier2[\"date\"] = pd.to_datetime(viollier2[\"date\"])\n",
    "\n",
    "viollier_city= {}\n",
    "for city in tqdm(cities_list, desc='Cities', position=0):\n",
    "\n",
    "\n",
    "viollierZH = viollier2[viollier2[\"canton\"]==\"ZH\"]\n",
    "viollierZH = viollierZH.sort_values(\"date\")\n",
    "viollierZH[\"frac\"] = viollierZH[\"b117\"] / viollierZH[\"sequenced\"]\n",
    "viollierZH = viollierZH[(viollierZH[\"date\"] >= np.datetime64(\"2020-12-08\")) & (viollierZH[\"date\"] <= np.datetime64(\"2021-02-11\"))]\n",
    "\n",
    "lowess = sm.nonparametric.lowess\n",
    "viollierZH[\"loess\"] = lowess(endog=viollierZH[\"frac\"],\n",
    "                             exog=np.arange(viollierZH.shape[0]).astype(\"float64\"),\n",
    "                             xvals=np.arange(viollierZH.shape[0]).astype(\"float64\"),\n",
    "                             frac= 2./3, it=3)\n",
    "\n",
    "viollierVD = viollier2[viollier2[\"canton\"]==\"VD\"]\n",
    "viollierVD = viollierVD.sort_values(\"date\")\n",
    "viollierVD[\"frac\"] = viollierVD[\"b117\"] / viollierVD[\"sequenced\"]\n",
    "viollierVD = viollierVD[(viollierVD[\"date\"] >= np.datetime64(\"2020-12-08\")) & (viollierVD[\"date\"] <= np.datetime64(\"2021-02-11\"))]\n",
    "\n",
    "lowess = sm.nonparametric.lowess\n",
    "viollierVD[\"loess\"] = lowess(endog=viollierVD[\"frac\"],\n",
    "                             exog=np.arange(viollierVD.shape[0]).astype(\"float64\"),\n",
    "                             xvals=np.arange(viollierVD.shape[0]).astype(\"float64\"),\n",
    "                             frac= 2./3, it=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viollier2[\"date\"] = pd.to_datetime(viollier2[\"date\"])\n",
    "\n",
    "viollierZH = viollier2[viollier2[\"canton\"]==\"ZH\"]\n",
    "viollierZH = viollierZH.sort_values(\"date\")\n",
    "viollierZH[\"frac\"] = viollierZH[\"b117\"] / viollierZH[\"sequenced\"]\n",
    "viollierZH = viollierZH[(viollierZH[\"date\"] >= np.datetime64(\"2020-12-08\")) & (viollierZH[\"date\"] <= np.datetime64(\"2021-02-11\"))]\n",
    "\n",
    "lowess = sm.nonparametric.lowess\n",
    "viollierZH[\"loess\"] = lowess(endog=viollierZH[\"frac\"],\n",
    "                             exog=np.arange(viollierZH.shape[0]).astype(\"float64\"),\n",
    "                             xvals=np.arange(viollierZH.shape[0]).astype(\"float64\"),\n",
    "                             frac= 1./3, it=3)\n",
    "\n",
    "viollierVD = viollier2[viollier2[\"canton\"]==\"VD\"]\n",
    "viollierVD = viollierVD.sort_values(\"date\")\n",
    "viollierVD[\"frac\"] = viollierVD[\"b117\"] / viollierVD[\"sequenced\"]\n",
    "viollierVD = viollierVD[(viollierVD[\"date\"] >= np.datetime64(\"2020-12-08\")) & (viollierVD[\"date\"] <= np.datetime64(\"2021-02-13\"))]\n",
    "\n",
    "lowess = sm.nonparametric.lowess\n",
    "viollierVD[\"loess\"] = lowess(endog=viollierVD[\"frac\"],\n",
    "                             exog=np.arange(viollierVD.shape[0]).astype(\"float64\"),\n",
    "                             xvals=np.arange(viollierVD.shape[0]).astype(\"float64\"),\n",
    "                             frac= 1./3, it=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample clinical loess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remake clinical loess by resampling cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viollier2 = { }\n",
    "for city in cities_list\n",
    "viollierZH2 = viollierZH[(viollierZH[\"date\"] >= np.datetime64(\"2020-12-08\")) & (viollierZH[\"date\"] <= np.datetime64(\"2021-02-11\"))]\n",
    "viollierVD2 = viollierVD[(viollierVD[\"date\"] >= np.datetime64(\"2020-12-08\")) & (viollierVD[\"date\"] <= np.datetime64(\"2021-02-13\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create extended df with one row per case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZH1hotdfs = []\n",
    "for d in viollierZH2[\"date\"]:\n",
    "    for i in range(viollierZH2[viollierZH2[\"date\"]==d][\"b117\"].values[0]):\n",
    "        ZH1hotdfs.append(pd.DataFrame({\"date\":d, \"wt\":0, \"b117\":1}, index=[0]))\n",
    "    for i in range(viollierZH2[viollierZH2[\"date\"]==d][\"sequenced\"].values[0] - viollierZH2[viollierZH2[\"date\"]==d][\"b117\"].values[0]):\n",
    "        ZH1hotdfs.append(pd.DataFrame({\"date\":d, \"wt\":1, \"b117\":0}, index=[0]))\n",
    "viollierZH2_1hot = pd.concat(ZH1hotdfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VD1hotdfs = []\n",
    "for d in viollierVD2[\"date\"]:\n",
    "    for i in range(viollierVD2[viollierVD2[\"date\"]==d][\"b117\"].values[0]):\n",
    "        VD1hotdfs.append(pd.DataFrame({\"date\":d, \"wt\":0, \"b117\":1}, index=[0]))\n",
    "    for i in range(viollierVD2[viollierVD2[\"date\"]==d][\"sequenced\"].values[0] - viollierVD2[viollierVD2[\"date\"]==d][\"b117\"].values[0]):\n",
    "        VD1hotdfs.append(pd.DataFrame({\"date\":d, \"wt\":1, \"b117\":0}, index=[0]))\n",
    "viollierVD2_1hot = pd.concat(VD1hotdfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resample cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinlowess = {}\n",
    "for city in tqdm(cities_list, desc='Cities'):\n",
    "    clinlowess[city] = []\n",
    "    np.random.seed(42)\n",
    "    for i in trange(1000):\n",
    "        resamp_df = viollierZH2_1hot.iloc[np.random.randint(0, viollierZH2_1hot.shape[0], viollierZH2_1hot.shape[0]),:]\n",
    "    resamp_df = resamp_df.groupby(\"date\").agg(\"sum\").reindex(viollierZH2[\"date\"])\n",
    "    resamp_df[\"freq\"] = resamp_df[\"b117\"] / (resamp_df[\"b117\"] + resamp_df[\"wt\"])\n",
    "    ZHclinlowess.append(lowess(resamp_df[\"freq\"],\n",
    "                               np.arange(resamp_df.shape[0]).astype(\"float64\"),\n",
    "                               xvals=np.arange(resamp_df.shape[0]).astype(\"float64\"), frac=1./3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDclinlowess = []\n",
    "np.random.seed(42)\n",
    "for i in trange(1000):\n",
    "    resamp_df = viollierVD2_1hot.iloc[np.random.randint(0, viollierVD2_1hot.shape[0], viollierVD2_1hot.shape[0]),:]\n",
    "    resamp_df = resamp_df.groupby(\"date\").agg(\"sum\").reindex(viollierVD2[\"date\"])\n",
    "    resamp_df[\"freq\"] = resamp_df[\"b117\"] / (resamp_df[\"b117\"] + resamp_df[\"wt\"])\n",
    "    VDclinlowess.append(lowess(resamp_df[\"freq\"],\n",
    "                               np.arange(resamp_df.shape[0]).astype(\"float64\"),\n",
    "                               xvals=np.arange(resamp_df.shape[0]).astype(\"float64\"), frac=1./3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None): #, 'display.max_columns', None):\n",
    "   newcolumn_data=df_cooc.loc[(df_cooc['amplicon'] == 91) & (df_cooc['IN2']==1)]\n",
    "   newcolumn_data.loc[((newcolumn_data['mut_all'] < 5) | (newcolumn_data['frac'] < 0.001))].loc['frac']=np.NaN\n",
    "   display(newcolumn_data.loc[df_wide['IN2'].index,'frac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio1",
   "language": "python",
   "name": "bio1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
